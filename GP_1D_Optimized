require(MASS)
require(ggplot2)
require(reshape2)

setwd("...")


# observed values
x <- c(-4,-3, -2, -1 , 1)
#y = (x)*(x-4) +2
y = sin(x)
#y = (x-1)*(x-3)


x.star <- seq(-5,5,len=100) # x values for prediction 
mu.x <- mean(x) # mean of observed x values

######## some GP functions sampled from a GP prior with SE kernel

# Calculates a Gaussian Kernel
#
# Parameters:
#    two vectors X1,X2 
#    l = bandwidth ^1/2
# Returns:
# 	a covariance matrix
getGaussian <- function(X1, X2 ,l=1, sigma.f = 1 ) {
  Zero <- matrix( rep( 0, len=length(X1)*length(X2)), nrow = length(X1))
  A <- Zero + X1
  B <- t(t(Zero)+X2)
  Sigma <- (sigma.f^2)*exp(-((A-B)^2)/(2*(l^2)))
  return(Sigma)
}


likelihood <- function(hyper, X = x, Y = y){
  # Computes marg. log-likelihood for a joint 
  # normal distribution of a Gaussian Process
  # with conditions on the kernel parameters
  # (have to be larger or equal zero),
  # i.e. how high is the likelihood given
  # a kernel with this parameter calibration
  # Note: requires function getGaussian() 
  #
  # Args:
  #   hyper:  vector of the transformed 
  #           hyperparameters of the kernel 
  #           matrix of the  observed input 
  #           values x, in the following 
  #           order: 
  #           log(sigma.y^2) , log(l^2) 
  #           log(sigma.f^2)
  #   x:      observed input values
  #   y:      observed output values
  # Returns:
  #   loglik : marginal log-likelihood 
  #            as scalar
  #
  length <- (exp(hyper[1]))^0.5 
  sigma.f <-(exp(hyper[2]))^0.5
  N <- length(X)
  K <- getGaussian(X,X, l = length, sigma.f=sigma.f)
  Ki <- chol2inv(chol(K))
  #Kyi <- chol2inv(chol(Ky)) # change this
  loglik <- 0.5*t(Y)%*%Ki%*%Y+ 0.5*log(det(K)) + N*0.5*log(2*pi)
  #loglik <- -0.5*Y%*%Kyi%*%Y - 0.5*log(det(Ky))-N*0.5*log(2*pi)
  return(loglik)
}
# optimize gradient parameters with optimize function
hyper.opt <- optim( par =  c(-2.3,-2.3), 
                    fn =  likelihood, 
                    method = c("CG")) 
hyper.opt
# step size is a problem... takes extremely small values for length parameter...
# solution: optimize l^2 for each input parameter?

length.opt <- exp(hyper.opt$par[1])^0.5 
sigma.f.opt <- exp(hyper.opt$par[2])^0.5

############# GP with conditioning on values
# predicting f(x)

K <- getGaussian(x, x, l=length.opt, sigma.f = sigma.f.opt)
K.star <- getGaussian(x, x.star, l=length.opt, sigma.f = sigma.f.opt) 
K.star.star <- getGaussian(x.star, x.star, l=length.opt, sigma.f = sigma.f.opt) # exp(-8)*diag(length(x.star)) #what is this?
postCov <- K.star.star - t(K.star)%*%solve(K)%*%K.star
mu.star <- t(K.star)%*%solve(K)%*%y

n.samples <- 6
values <- matrix(rep(0,length(x.star)*n.samples), ncol=n.samples)
for (i in 1:n.samples) {
  values[,i] <- mvrnorm(1, mu.star , postCov)
}
values <- cbind(x=x.star,as.data.frame(values))
values <- melt(values,id="x")

S2 <- diag(postCov)
lowerbound <- mu.star +2*sqrt(S2)
upperbound <- mu.star -2*sqrt(S2)
gauss.no.error <- ggplot() + 
  geom_ribbon(x = x.star, aes(ymin= lowerbound, 
                              ymax = upperbound), fill = "grey70") +
  geom_point( data = NULL,aes( x = x, y=y)) +
  geom_line(data = values, aes(x=x,y=value, 
                               group=variable, colour = variable)) +
  theme(legend.position="none")
#geom_point( aes(x= x.star, y=mu.star)) +

gauss.no.error
ggsave("gpnoerror.pdf", plot = gauss.no.error ) # save
