#################################################

#  Gaussian Process on Boston Housing Prices with
#  Conjugate Gradient Optimization on the Kernel
#  Parameters
#################################################

# Structure:
# 0. Load Packages and Data
# 1. Choosing Input and Output Variables
# 2. Kernel Parameter Optimization
# 3. Computing Predictions
# 4. Evaluating Predictions

# set working directory
setwd("/Users/claracharlottehoffmann/Desktop/MachineLearning")

##################################################

# 0.  Load Packages and Data
##################################################

# load packages
if (!require("pacman")) 
  install.packages("pacman"); library("pacman") 
p_load("MASS", 
       "ggplot2", 
       "reshape2",
       "mlbench", 
       "plotly", 
       "pdist",
       "gpr",
       "psych",
       "pbdDMAT")

#  read in data
data(BostonHousing2)
data <- BostonHousing2

##################################################

# 1. Choosing Input and Output Variables
##################################################

#  random subset as training and test data
subset <- sample(1:506, 106, replace=F)
train<- data[-(subset),]  
test <- data[subset,]

# define x and f(x) = y
# from training set
x <- train[,c(4,5)] #,7,9,11,12,13,14,15,17,18,19,20,21
y <- train$medv
# define x values at which to predict
# from test set
x.star <-  test[,c(4,5)] #,7,9,11,12,13,14,15,17,18,19,20,21

##################################################

# 2. Kernel Parameter Optimization
##################################################

getGaussian <- function(X1, X2 = NULL ,l=1, sigma.f ) {
  # Computes gaussian kernel for matrices/vectors
  # Allows calibration of all kernel parameters
  # Option for two or one matrices/vectors
  #
  # Args:
  #   X1:      first/only matrix or vector
  #   X2:      second matrix or vector
  #   l:       scaling parameter in exponent
  #   sigma.f: scaling parameter in base
  #
  # Returns:
  #   gaussian kernel matrix
  #
  # calculate distance between all elements
  dist <- switch( is.null(X2) + 1, 
                  as.matrix(pdist(X1, X2)),
                  as.matrix(dist(X1)))
  #compute kernel
  Sigma <- (sigma.f^2)*exp(-0.5*(dist^2/(l^2)))
  return(Sigma)
}

getGaussian(X1 = x, l = 5, sigma.f = -10) # just for testing function

likelihood <- function(hyper, X = x, Y = y){
  # Computes marg. log-likelihood for a joint 
  # normal distribution of a Gaussian Process
  # with conditions on the kernel parameters
  # (have to be larger or equal zero),
  # i.e. how high is the likelihood given
  # a kernel with this parameter calibration
  # Note: requires function getGaussian() 
  #
  # Args:
  #   hyper:  vector of the transformed 
  #           hyperparameters of the kernel 
  #           matrix of the  observed input 
  #           values x, in the following 
  #           order: 
  #           log(sigma.y^2) , log(l^2) 
  #           log(sigma.f^2)
  #   x:      observed input values
  #   y:      observed output values
  # Returns:
  #   loglik : marginal log-likelihood 
  #            as scalar
  #
  noise <-  (exp(hyper[1]))^0.5
  length <- (exp(hyper[2]))^0.5 
  sigma.f <-(exp(hyper[3]))^0.5
  N <- nrow(X)
  K <- getGaussian(X1 = X, l = length, sigma.f=sigma.f)
  Ky <- K + diag( nrow=ncol(K))*(noise^2)
  Kyi <- chol2inv(chol(Ky))
  loglik <- -0.5*Y%*%Kyi%*%Y - 0.5*log(det(Ky))-N*0.5*log(2*pi)
  return(loglik)
}

grad <- function(hyper, X = x, Y = y){
  # Computes gradient of the marg. log-likelihood 
  # for a joint normal distribution of a Gaussian 
  # Process with conditions on the kernel 
  # parameters (have to be larger or equal zero)
  #
  # Args:
  #   hyper:  vector of the transformed 
  #           hyperparameters of the kernel 
  #           matrix of the  observed input 
  #           values x, in the following 
  #           order: 
  #           log(sigma.y^2) , log(l^2) 
  #           log(sigma.f^2)
  #   X:      observed input values
  #   Y:      observed output values
  # Returns:
  #   gradient : 3x1 vector, containing the 
  #             derivations of the likelihood
  #             wrt. the transforrmed kernel 
  #             parameters in the following 
  #             order:
  #             log(sigma.y^2) , log(l^2) 
  #             log(sigma.f^2)
  #             
  # transform kernel parameters back
  noise <- (exp(hyper[1]))^0.5 
  length <- (exp(hyper[2]))^0.5
  sigma.f <- (exp(hyper[3]))^0.5
  # compute kernel and inverse kernel
  K <- getGaussian(X1 = X, l = length, 
                   sigma.f=sigma.f)
  Ky <- K + diag( nrow=ncol(K))*(noise^2) 
  Kyi <- chol2inv(chol(Ky))
  const <- Kyi%*%Y%*%Y%*%t(Kyi) - Kyi
  # compute gradient elements
  delta.noise <- 0.5 * tr(const*(diag( 
    nrow=ncol(K))*noise)) 
  delta.sigma.f <- 0.5 * tr(const*2*Ky/sigma.f)
  dist <- as.matrix(dist(X))
  # check this derivation...
  delta.length <- 0.5 * tr(const*(
    Ky*(dist^2)/(length^2)))
  gradient <- c(delta.noise, 
                delta.sigma.f, 
                delta.length )
  return(gradient)
}
#test function -> delete later
hyper <- c(0.01,-0.05, 1)
likelihood(hyper)
grad(hyper)

# optimize gradient parameters with optimize function
hyper.opt <- optim( par =  c(0.01,-0.05, 1), 
                    fn =  likelihood, gr = grad,
       method = c("CG")) # "CG"
# step size is a problem... takes extremely small values for length parameter...
# solution: optimize l^2 for each input parameter?

noise.opt <- exp(hyper.opt$par[1])^0.5
length.opt <- exp(hyper.opt$par[1])^0.5 
sigma.f.opt <- exp(hyper.opt$par[1])^0.5


##################################################

# 3. Computing Predictions
##################################################

# compute covariance matrix of GP:
#  K(x,x) = Ky          K(x,x*) = K.star
#  K(x,x*)' = K.star'  K(x*,x*) = K.star.star
noise <- noise.opt
K <- getGaussian(X1 = x, l = length.opt, 
                 sigma.f=sigma.f.opt)
Ky <- K + diag( nrow=ncol(K))*(noise.opt^2)
K.star <- getGaussian(x, x.star, l = length.opt, 
                      sigma.f=sigma.f.opt)
K.star.star <- getGaussian(X1 = x.star, 
                           l = length.opt, 
                           sigma.f=sigma.f.opt) 
postCov <- K.star.star - 
  t(K.star)%*%chol2inv(chol((Ky)))%*%K.star
#  400x400     - 400x106    106x106        106x400
# compute mean prediction s
mu.star <- t(K.star)%*%chol2inv(chol((Ky)))%*%y
# bind data
predictions <- cbind(x.star, y = mu.star, pred = 1)
training <- cbind(x, pred = 0)
results <- rbind(cbind(training, y), predictions)

##################################################

# 4. Evaluating Predictions
##################################################

# produce plot of "true" values from test set
# on x-axis against predicted values from GP
# on y-axis
pred.gp <- ggplot(data = NULL, 
                  aes(x = test$medv, 
                      y = predictions$y)) + 
  geom_point() +
  scale_x_continuous(name = "True Values") +
  scale_y_continuous(name = "GP Predictions") +
  ggtitle("Prediction of House Prices 
          based on Longitude and Latitude")
pred.gp  # show plot
ggsave("prediction.jpg", plot = pred.gp )  # save
