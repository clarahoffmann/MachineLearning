
x <- c(-4,-3, -2, -1 , 1)
noise <- 0.1
y = sin(x) + mvrnorm(1, 0 , noise)
x.star <- seq(-5,5,len=100) # x values for prediction 

# Calculates a Gaussian Kernel
#
# Parameters:
#    two vectors X1,X2 
#    l = bandwidth ^1/2
# Returns:
# 	a covariance matrix
getGaussian <- function(X1, X2 ,l=1, sigma.f = 1 ) {
  Zero <- matrix( rep( 0, len=length(X1)*length(X2)), nrow = length(X1))
  A <- Zero + X1
  B <- t(t(Zero)+X2)
  Sigma <- (sigma.f^2)*exp(-((A-B)^2)/(2*(l^2)))
  return(Sigma)
}

#alternative Gaussian kernel function for vectors
getGaussian2 <- function(X1,X2=NULL) {
  #Sigma <- (sigma.f^2)*exp(-((A-B)^2)/(2*(l^2)))
  return(as.matrix(rdist(X1,X2)))
}

likelihood <- function(hyper, X = x, Y = y){
  # Computes marg. log-likelihood for a joint 
  # normal distribution of a Gaussian Process
  # with conditions on the kernel parameters
  # (have to be larger or equal zero),
  # i.e. how high is the likelihood given
  # a kernel with this parameter calibration
  # Note: requires function getGaussian() 
  #
  # Args:
  #   hyper:  vector of the transformed 
  #           hyperparameters of the kernel 
  #           matrix of the  observed input 
  #           values x, in the following 
  #           order: 
  #           log(sigma.y^2) , log(l^2) 
  #           log(sigma.f^2)
  #   x:      observed input values
  #   y:      observed output values
  # Returns:
  #   loglik : marginal log-likelihood 
  #            as scalar
  #
  noise <-  (exp(hyper[1]))^0.5
  length <- (exp(hyper[2]))^0.5 
  sigma.f <-(exp(hyper[3]))^0.5
  N <- length(X)
  K <- getGaussian(X,X, l = length, sigma.f=sigma.f)
  Ky <- K + diag( nrow=ncol(K))*(noise^2)
  Kyi <- chol2inv(chol(Ky))
  loglik <- 0.5*t(Y)%*%Kyi%*%Y+ 0.5*log(det(Ky)) + N*0.5*log(2*pi)
  return(loglik)
}

# optimize gradient parameters with optimize function
hyper.opt <- optim( par =  c(1,1,1), 
                    fn =  likelihood, 
                    method = c("CG")) 
hyper.opt
# extract optimal hyperparameters
noise.opt <- exp(hyper.opt$par[1])^0.5
length.opt <- exp(hyper.opt$par[2])^0.5 
sigma.f.opt <- exp(hyper.opt$par[3])^0.5
# compute covariance matrix and posterior means
K.star <- getGaussian(x, x.star,l=length.opt, sigma.f = sigma.f.opt) 
K.star.star <- getGaussian(x.star, x.star,l=length.opt, sigma.f = sigma.f.opt) # exp(-8)*diag(length(x.star)) #what is this?
Ky <- getGaussian(x, x,l=length.opt, sigma.f = sigma.f.opt) + diag( nrow=length(x))*noise.opt^2
postCov <- K.star.star - t(K.star)%*%solve(Ky)%*%K.star
mu.star <- t(K.star)%*%solve(Ky)%*%y 


n.samples <- 6
values <- matrix(rep(0,length(x.star)*n.samples), ncol=n.samples)
for (i in 1:n.samples) {
  values[,i] <- mvrnorm(1, mu.star , postCov)
}
values <- cbind(x=x.star,as.data.frame(values))
values <- melt(values,id="x")

S2 <- diag(postCov)
lowerbound <- mu.star +2*sqrt(S2)
upperbound <- mu.star -2*sqrt(S2)
gauss.error <- ggplot() + 
  geom_ribbon(x = x.star, aes(ymin= lowerbound, 
                              ymax = upperbound), fill = "grey70") +
  geom_point( data = NULL,aes( x = x, y=y)) +
  geom_line(data = values, aes(x=x,y=value, group=variable, color=variable)) +
  theme(legend.position="none")
gauss.error
ggsave("gpwitherror.pdf", plot = gauss.error ) # save
